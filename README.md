# AIEA-auditor-task-8
This repository contains an implementation of the symbol logic LLM discussed in this paper: https://arxiv.org/abs/2305.12295 to interpret natural language logic using translation to first order logic. The alogirthm first runs natural language text through an LLM through langchain and then runs prover9 on it to factcheck it: https://www.cs.unm.edu/~mccune/prover9/ (this part not yet implemented), and then runs it through an entailment result interpreter (please review research paper attached above) to translate FOL back into NL (also not implemented yet). Here is an image of a visualization of this process (note: the highlighted part is relavant for First Order Logic). For more information, please review the research paper linked above. 
<img width="742" alt="image" src="https://github.com/user-attachments/assets/03611547-0164-47a7-a0a1-b4a43f2cef29" />

Tutorial:
To run the logic llm paper implementation, update thek knowledge base file with the desired knowledgebase and update the IMPORTANT QUERY PATTERNS section of the system_message string and the desired query in the content_message string, then run the paper_implementation.py file. This file operates in the 4 steps listed in the message. A tool to query the knowledgebase is passed to the LLM model along with the query (problem formulator), The LLM then generates a tool call with the desired prolog query, which the LLM gets the response from the knowledge base (First order logic prover), and then the prolog response is translated into natural language and returned as the LLM response (entailment stage)

To run the rag implementation, update the knowledge base file, the raw_documents string in the rag.py file with desired relationships form your new knowledgebase, update the prompt string with relevant examples from the updated knowledgebase, run the file, enter your query as user input when prompted and the LLM model will use RAG to give the correct response to the query (assuming the query can be answered using the knowledgebase). We used In memory vector store due to its simplicity with the gpt 4o mini embedded model due to its versatality. Our documents include snippets of our knowledge base translated into natural language. 
